{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import tensorflow package for modeling\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "## Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## Plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Initializing module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "## Copy module\n",
    "import copy\n",
    "\n",
    "## Used to calculate the training time\n",
    "import time\n",
    "\n",
    "## Set the GUP environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/Oring-AI/Model_I/DINKLE/t+4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the display\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control memory usage space for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前設備： 0\n",
      "目前設備名： GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "## 查詢有無可用 GPU\n",
    "torch.cuda.is_available()\n",
    "## 查詢可用 GPU 的數量\n",
    "torch.cuda.device_count()\n",
    "##目前設備\n",
    "print(\"目前設備：\",torch.cuda.current_device())\n",
    "## 目前設備名\n",
    "print(\"目前設備名：\",torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cacl(pred_value, actual_value):\n",
    "    \n",
    "#     yo, loss, tape = network.forward()\n",
    "    performance = []\n",
    "    performance.append(torch.mean(torch.abs(pred_value - actual_value)))\n",
    "    performance.append(torch.std(torch.abs(pred_value - actual_value)))\n",
    "    performance.append(torch.mean(torch.abs((pred_value - actual_value) / actual_value))) \n",
    "    performance.append(torch.sqrt(torch.mean((pred_value - actual_value)**2)))\n",
    "    \n",
    "    for i in range(2000,3001,1000):\n",
    "        correct_times = torch.nonzero(torch.abs(pred_value - actual_value) <= i, as_tuple =False)\n",
    "        accuracy = correct_times.shape[0]/pred_value.shape[0]\n",
    "        performance.append(accuracy)\n",
    "                       \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(name, pred_value, actual_value,block_index):\n",
    "    \n",
    "#     fig, ax = plt.subplots(2,2,figsize=(20,10), sharex=True, sharey=True)\n",
    "    fig, ax = plt.subplots(1,figsize=(20,10), sharex=True, sharey=True)\n",
    "#     ax.set_xlim(0,pred_value.shape[0])  \n",
    "    \n",
    "    \n",
    "    \n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.plot(pred_value, label=\"LLAAT\")\n",
    "    ax.plot(actual_value, label=\"Actual\")\n",
    "    ax.set_title(\"Forecasted performance for l=%d\" %(1))\n",
    "    ax.legend()\n",
    "        \n",
    "    #fig.text(0.5, 0, \"Stage of training\", ha='center', fontsize=20)\n",
    "    #fig.text(0, 0.5, \"Copper price value\", va='center', rotation='vertical')\n",
    "\n",
    "    fig.suptitle(\"In the %s process in the M=%d window\"%(name, block_index))\n",
    "    fig.tight_layout()\n",
    "#     fig.savefig(\"In the %s process in the M=%d window.png\"%(name, block_index),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adopted_node(network,block_index):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "#     ax.set_xticklabels([i for i in range(network.nb_node_acceptable.shape[0]+5)])\n",
    "    \n",
    "    ax.set_title(\"Total amount of adopted hidden nodes in the training process in the M=%d window\"%(block_index))\n",
    "    ax.plot(network.nb_node_acceptable,\"-o\")\n",
    "\n",
    "    ax.set_xlabel(\"Stage of training\")\n",
    "    ax.set_ylabel(\"Hidden nodes\")\n",
    "    \n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "#     fig.savefig(\"hidden nodes in the training process in the M=%d window\"%(block_index),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_table(evaluation_results, block_index, name, performance, nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node):\n",
    "\n",
    "    \n",
    "    new_result = pd.DataFrame({\n",
    "\n",
    "        \"Window_index\":block_index,\n",
    "        \"Stage\":name,\n",
    "        \"MAE\" : round(performance[0].item(),2),\n",
    "        \"SD of absolute error\":round(performance[1].item(),2),\n",
    "        \"MAPE\" : \"%.2f\"%(performance[2]*100).item(),\n",
    "        \"RMSE\" : round(performance[3].item(),2),\n",
    "#         \"Accuracy(2000)\" : [round(performance[4]*100,2)],\n",
    "        \"Accuracy(3000)\" : [round(performance[5]*100,2)],\n",
    "        \"Step4\":nb_step4,\n",
    "        \"Step6.1\":nb_step6_1,\n",
    "        \"Step6.2\":nb_step6_2,\n",
    "        \"Time\":time,\n",
    "        \"Adopted_hidden_node\":adopted_hidden_node\n",
    "    })\n",
    "\n",
    "    evaluation_results = evaluation_results.append(new_result, ignore_index=True)\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(network, nb_step4, nb_step6_1, nb_step6_2, x_train_scaled, y_train_scaled, x_test, y_test, start, end, block_index, evaluation_results_train, evaluation_results_test):\n",
    "\n",
    "    ## Training_Step\n",
    "#    print(\"<<Training step>>\")\n",
    "    print(\"The training time(s):\",end - start)\n",
    "    time = end - start\n",
    "    yo, loss= network.forward()\n",
    "    \n",
    "    ## N - outlier\n",
    "    pre_train = yo.data.cpu()\n",
    "    true_train = network.y.data.cpu()\n",
    "    \n",
    "    pred_value_train = torch.FloatTensor(sc.inverse_transform(pre_train))\n",
    "    actual_value_train = torch.FloatTensor(sc.inverse_transform(true_train))\n",
    "    accuracy_train = accuracy_cacl(pred_value_train,actual_value_train)\n",
    "\n",
    "    ## B\n",
    "    pred_value_test = torch.FloatTensor(sc.inverse_transform(network.forecast(x_test).data.cpu()))\n",
    "    accuracy_test = accuracy_cacl(pred_value_test, y_test)\n",
    "    \n",
    "#     total_time = nb_step4 + nb_step6_1 + nb_step6_2\n",
    "#     print(\"<<The percentage of each step>>\")\n",
    "#     print(\"Step 4: %.2f%%\"%((nb_step4/total_time)*100))\n",
    "#     print(\"Step 6.1: %.2f%%\"%((nb_step6_1/total_time)*100))\n",
    "#     print(\"Step 6.2: %.2f%%\"%((nb_step6_2/total_time)*100))\n",
    "    \n",
    "#     print(\"-\"*60)\n",
    "#     print(\"Total frequency of cramming occurrences:\",nb_step6_2)\n",
    "\n",
    "#     print(\"-\"*60)\n",
    "#     print(\"The amount of hidden node that be pruned:\",network.nb_node_pruned)\n",
    "\n",
    "#     print(\"-\"*60)\n",
    "    adopted_hidden_node = network.nb_node_acceptable[-1].item()\n",
    "#     print(\"The amount of adopted hidden nodes:\",network.nb_node_acceptable[-1].item())\n",
    "\n",
    "#     print(\"-\"*60)\n",
    "#     print(\"<<Accuracy in training step>>\")\n",
    "#     print(\"The MAE for l = 1: %.2f\" %(accuracy_train[0]))\n",
    "#     print(\"The MAPE for l = 1: %.2f%%\" %(accuracy_train[1]))\n",
    "#     print(\"The RMSE for l = 1: %.2f\" %(accuracy_train[2]))\n",
    "#     print(\"The accuracy(2000) for l = 1: %.2f%%\" %(accuracy_train[3]*100))\n",
    "#     print(\"The accuracy(3000) for l = 1: %.2f%%\" %(accuracy_train[4]*100))\n",
    "\n",
    "    print(\"The forecast copper price for the fourth week:\", pred_value_test[-4:].tolist())\n",
    "    print(\"<<Performance measurement>>\")\n",
    "    print(\"The MAE: %.1f\" %(accuracy_test[0]))\n",
    "    print(\"The standard deviation of absolute error: %.1f\" %(accuracy_test[1]))\n",
    "    print(\"The MAPE: %.1f%%\" %(accuracy_test[2]))\n",
    "    print(\"The RMSE: %.1f\" %(accuracy_test[3]))\n",
    "#    print(\"The accuracy(2000) for l = 1: %.1f%%\" %(accuracy_test[4]*100))\n",
    "    print(\"The accuracy(3000): %.1f%%\" %(accuracy_test[5]*100))\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    \n",
    "    evaluation_table_train = evaluation_table(evaluation_results_train, block_index, \"Training\", accuracy_train,nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node)\n",
    "    evaluation_table_test = evaluation_table(evaluation_results_test, block_index, \"Inferencing\", accuracy_test,nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node)\n",
    "    pre_LDSS = sc.inverse_transform(network.forecast(x_test).data.cpu())\n",
    "#     pd.DataFrame(pre_LDSS).to_csv(\"pre_LDSS_%d.csv\"%(block_index), index=False)\n",
    "    \n",
    "#     plot_result(\"training\",pred_value_train, actual_value_train,block_index)\n",
    "#     plot_result(\"inferencing\",pred_value_test, y_test,block_index)\n",
    "#     plot_adopted_node(network,block_index)\n",
    "    \n",
    "    return(evaluation_table_train, evaluation_table_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek=4, futureWeek=4, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek):\n",
    "        X = np.array(train.iloc[i:i+defaultWeek])\n",
    "        X = np.append(X,train[\"CCSP\"].iloc[i+defaultWeek:i+pastWeek])\n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use min-max normalization to scale the data to the range from 1 to 0\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design get_data() to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(futureWeek):\n",
    "    \n",
    "    ## Read weekly copper price data\n",
    "    path = \"WeeklyFinalData.csv\"\n",
    "    data = read(path)\n",
    "    \n",
    "    date = data[\"Date\"]\n",
    "    data.drop(\"Date\", axis=1, inplace=True)\n",
    "    \n",
    "    ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "    x_data, y_data = buildTrain(data, futureWeek=futureWeek)\n",
    "\n",
    "\n",
    "    return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, nb_neuro, x_train_scaled, y_train_scaled):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(x_train_scaled.shape[1], nb_neuro).cuda()\n",
    "        self.linear2 = torch.nn.Linear(nb_neuro, 1).cuda()\n",
    "        \n",
    "        \n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 0.12\n",
    "        self.threshold_for_lr = 1e-4\n",
    "        \n",
    "        # Input data \n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "        \n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-3\n",
    "        \n",
    "        # Whether the network is acceptable, default as False\n",
    "        self.acceptable = False\n",
    "        \n",
    "        # Some record for experiment\n",
    "        self.nb_node_pruned = 0\n",
    "        self.nb_node_acceptable=torch.IntTensor([nb_neuro])\n",
    "        \n",
    "        self.limit = nb_neuro\n",
    "        \n",
    "    ## Forecast the test data\n",
    "    def forecast(self, x_test):\n",
    "    \n",
    "        x_test = torch.FloatTensor(x_test).cuda()\n",
    "        activation_value = self.linear1(x_test).clamp(min=0)\n",
    "        forecast_value = self.linear2(activation_value)\n",
    "       \n",
    "        return forecast_value\n",
    "\n",
    "    ## Reset the x and y data\n",
    "    def setData(self, x_train_scaled, y_train_scaled):\n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "    \n",
    "    ## Add the new data to the x and y data\n",
    "    def addData(self, new_x_train, new_y_train):\n",
    "\n",
    "        self.x = torch.cat([self.x, new_x_train.reshape(1,-1).cuda()],0)\n",
    "        self.y = torch.cat([self.y, new_y_train.reshape(-1,1).cuda()],0)\n",
    "    \n",
    "    ## forward operation\n",
    "    def forward(self, reg_strength=0):\n",
    "       \n",
    "        y1 = self.linear1(self.x).clamp(min=0)\n",
    "        yo = self.linear2(y1)\n",
    "\n",
    "        # performance measure\n",
    "        param_val= torch.sum(torch.pow(self.linear2.bias.data,2))+torch.sum(torch.pow(self.linear2.weight.data,2))+torch.sum(torch.pow(self.linear1.bias.data,2))+torch.sum(torch.pow(self.linear1.weight.data,2))\n",
    "        reg_term= reg_strength/((self.linear2.bias.data.shape[0]*(self.linear2.weight.data.shape[1]+1)) +(self.linear1.bias.data.shape[0]*(self.linear1.weight.data.shape[1]+1)))*param_val\n",
    "        loss = torch.nn.functional.mse_loss(yo,self.y)+reg_term\n",
    "        loss = loss.cuda()\n",
    "        return(yo, loss)\n",
    "\n",
    "    # backward operation\n",
    "    def backward_Adadelta(self,loss):    \n",
    "\n",
    "        optimizer = optim.Adadelta(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializing(network, initial_x, initial_y):\n",
    "#     print(\"Initializing module\")\n",
    "    ## Find each minimum output value y\n",
    "    min_y = torch.min(initial_y, axis=0)\n",
    "    ## Subtract min_y from each y\n",
    "    res_y = initial_y-min_y.values\n",
    "    \n",
    "    ## Use linear regression to find the initial W1,b1,Wo,bo\n",
    "    reg = LinearRegression().fit(initial_x, res_y)\n",
    "    \n",
    "    ## Set up the initial parameter of the network\n",
    "    network.linear1.weight = torch.nn.Parameter(torch.FloatTensor(reg.coef_).cuda())\n",
    "    network.linear1.bias = torch.nn.Parameter(torch.FloatTensor(reg.intercept_).cuda())\n",
    "    network.linear2.weight=torch.nn.Parameter(torch.FloatTensor([[1]]).cuda())\n",
    "    network.linear2.bias = torch.nn.Parameter(torch.FloatTensor(min_y.values).cuda())\n",
    "    \n",
    "#     print(reg.coef_)\n",
    "#     print(reg.intercept_)\n",
    "\n",
    "    ## Set up the acceptable of the initial network as True\n",
    "    network.acceptable =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting(network, x_train_scaled, y_train_scaled):\n",
    "    \n",
    "#     print(\"<<Selecting module>>\")\n",
    "    loss = []\n",
    "    temp_network = copy.deepcopy(network)\n",
    "    \n",
    "    ## Put each data into network to calculate the loss value\n",
    "    for i in range(x_train_scaled.shape[0]):\n",
    "        temp_network.setData(x_train_scaled[i].reshape(1,-1), y_train_scaled[i].reshape(-1,1))\n",
    "        loss.append((temp_network.forward()[1].item(),i))\n",
    "#         print(network.state_dict())\n",
    "#         print(temp_network.y)\n",
    "#         print(\"-\"*20)\n",
    "#         print(temp_network.forward()[1])\n",
    "#         print(\"-\"*20)\n",
    "#     ## Sort the data according to the loss value from smallest to largest, and save the data index in sorted_index\n",
    "    sorted_index = [sorted_data[1] for sorted_data in sorted(loss, key = lambda x:x[0])]\n",
    "    \n",
    "    \n",
    "    ## Print out some info for debug\n",
    "#     print(\"The loss value of k:\",loss[sorted_index[0]])\n",
    "#     print(\"The second_loss value of k:\",loss[sorted_index[1]])\n",
    "#     print(\"Selecting module finish!\")\n",
    "#     print(\"Loss\",loss)\n",
    "#     print(network.state_dict())\n",
    "    \n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(network):\n",
    "\n",
    "    times_enlarge=0\n",
    "    times_shrink=0\n",
    "    \n",
    "#     print(\"<<Matching module>>\")\n",
    "#     print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    network.acceptable = False\n",
    "    initial_network = copy.deepcopy(network)\n",
    "    yo, loss = network.forward()\n",
    "    \n",
    "    if torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "        network.acceptable = True\n",
    "#         print(\"Matching(o) first finished - the network is acceptable\")\n",
    "#         print(\"Number of enlarge:\",times_enlarge)\n",
    "#         print(\"Number of shrink:\",times_shrink)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(10000):\n",
    "            \n",
    "            yo, loss = network.forward()\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            loss_pre = loss\n",
    "\n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adadelta(loss)\n",
    "            yo, loss = network.forward()\n",
    "\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss <= loss_pre and torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.acceptable = True\n",
    "#                 print(\"Matching finished - the network is acceptable\")\n",
    "#                 print(\"Number of enlarge:\",times_enlarge)\n",
    "#                 print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "            elif loss <= loss_pre:\n",
    "                \n",
    "                times_enlarge+=1\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "\n",
    "            else:         \n",
    "\n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "#                     print(\"Matching finished - the network is Unacceptable\")\n",
    "#                     print(\"Number of enlarge:\",times_enlarge)\n",
    "#                     print(\"Number of shrink:\",times_shrink)\n",
    "                    return(initial_network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "                    # Restore the papameter of the network\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    times_shrink+=1\n",
    "                    network.learning_rate *= 0.7\n",
    "                \n",
    "        network.acceptable = False\n",
    "#         print(\"Matching的第%d回合\"%(i+1))\n",
    "#         print(\"Matching finished - the network is Unacceptable\")\n",
    "#         print(\"Number of enlarge:\",times_enlarge)\n",
    "#         print(\"Number of shrink:\",times_shrink)\n",
    "        return(initial_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching for reorganizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_for_reorganizing(network):\n",
    "\n",
    "    times_enlarge=0\n",
    "    times_shrink=0\n",
    "    \n",
    "#     print(\"<<Matching module for reorganizing>>\")\n",
    "#     print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    network.acceptable = False\n",
    "    initial_network = copy.deepcopy(network)\n",
    "    yo, loss = network.forward()\n",
    "    \n",
    "    if torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "        network.acceptable = True\n",
    "#         print(\"Matching(o) first finished - the network is acceptable\")\n",
    "#         print(\"Number of enlarge:\",times_enlarge)\n",
    "#         print(\"Number of shrink:\",times_shrink)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(500):\n",
    "            \n",
    "            yo, loss = network.forward()\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            loss_pre = loss\n",
    "\n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adadelta(loss)\n",
    "            yo, loss = network.forward()\n",
    "\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss <= loss_pre and torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.acceptable = True\n",
    "#                 print(\"Matching finished(o) - the network is acceptable\")\n",
    "#                 print(\"Number of enlarge:\",times_enlarge)\n",
    "#                 print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "            elif loss <= loss_pre:\n",
    "                \n",
    "                times_enlarge+=1\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "\n",
    "            else:         \n",
    "\n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "#                     print(\"Matching finished(o) - the network is Unacceptable\")\n",
    "#                     print(\"Number of enlarge:\",times_enlarge)\n",
    "#                     print(\"Number of shrink:\",times_shrink)\n",
    "                    return(initial_network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "                    \n",
    "                    # Restore the papameter of the network\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    times_shrink+=1\n",
    "                    network.learning_rate *= 0.7\n",
    "                \n",
    "        network.acceptable = False\n",
    "#         print(\"Matching的第%d回合\"%(i+1))\n",
    "#         print(\"Matching finished - the network is Unacceptable\")\n",
    "#         print(\"Number of enlarge:\",times_enlarge)\n",
    "#         print(\"Number of shrink:\",times_shrink)\n",
    "        return(initial_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cramming module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramming(network):\n",
    "    \n",
    "    torch.random.manual_seed(0)\n",
    "#     print(\"<<Cramming module>>\")\n",
    "#     print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Find unsatisfied data:K\n",
    "    yo, loss = network.forward()\n",
    "    undesired_index = torch.nonzero(torch.abs(yo-network.y) > network.threshold_for_error+0.001, as_tuple =False)\n",
    "\n",
    "    ## Print out the undesired_index for debug\n",
    "#     print(\"不滿足個數：\",undesired_index.shape[0])\n",
    "#     print(\"The index of the undesired data:\",undesired_index)\n",
    "\n",
    "    \n",
    "    if undesired_index.shape[0] == 1:\n",
    "        \n",
    "        # Unsatisfied situation\n",
    "        ## Find the index of the unsatisfied data\n",
    "        k_data_num = undesired_index[0][0]\n",
    "\n",
    "        undesired_data = torch.reshape(network.x[k_data_num,:], [1,-1])\n",
    "\n",
    "        ## Remove the data that does not meet the error term\n",
    "        left_data = network.x[:k_data_num,:]\n",
    "        right_data = network.x[k_data_num+1:,:]\n",
    "        remain_tensor = torch.cat([left_data, right_data], 0)\n",
    "\n",
    "\n",
    "        ## Use the random method to find out the gamma and zeta\n",
    "        while True:\n",
    "\n",
    "            ## Find m-vector gamma: r\n",
    "            ## Use the random method to generate the gamma that can make the conditions met\n",
    "            gamma = torch.rand(size=[1,network.x.shape[1]]).cuda()\n",
    "            subtract_undesired_data = torch.sub(remain_tensor, undesired_data)\n",
    "            matmul_value = torch.mm(gamma,torch.t(subtract_undesired_data))\n",
    "\n",
    "            if torch.all(matmul_value != 0):\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "\n",
    "            ## Find the tiny value: zeta\n",
    "            ## Use the random method to generate the zeta that can make the conditions met\n",
    "            zeta = torch.rand(size=[1]).cuda()\n",
    "\n",
    "            if torch.all(torch.mul(torch.add(zeta,matmul_value),torch.sub(zeta,matmul_value))<0):\n",
    "                break\n",
    "\n",
    "\n",
    "        k_l = undesired_index[0][1]\n",
    "        \n",
    "        ## The weight of input layer to hidden layer I\n",
    "        w10 = gamma\n",
    "        w11 = gamma\n",
    "        w12 = gamma\n",
    "\n",
    "        W1_new = torch.cat([w10,w11,w12],0)\n",
    "        \n",
    "\n",
    "        ## The bias of input layer to hidden layer I\n",
    "        matual_value = torch.mm(gamma,torch.t(undesired_data))\n",
    "       \n",
    "        \n",
    "        b10 = torch.sub(zeta,matual_value)\n",
    "        b11 = -1*matual_value\n",
    "        b12 = torch.sub(-1*zeta,matual_value)\n",
    "\n",
    "        b1_new = torch.reshape(torch.cat([b10,b11,b12],0),[3])\n",
    "\n",
    "        ## The weight of hidden layer I to output layer\n",
    "        gap = network.y[k_data_num, k_l]-yo[k_data_num, k_l]\n",
    "\n",
    "        wo0_value = gap/zeta\n",
    "        wo1_value = (-2*gap)/zeta\n",
    "        wo2_value = gap/zeta\n",
    "\n",
    "        Wo_new = torch.reshape(torch.cat([wo0_value,wo1_value,wo2_value],0),[1,-1])\n",
    "\n",
    "        ## Add new neuroes to the network\n",
    "        network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight.data, W1_new]))\n",
    "        network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias.data, b1_new]))\n",
    "        network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight.data, Wo_new],1))\n",
    "\n",
    "\n",
    "        yo, loss = network.forward()\n",
    "        \n",
    "        ## Determine if cramming is successful and print out the corresponding information\n",
    "        if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "            network.acceptable = True \n",
    "#             print(\"Cramming success!\")\n",
    "\n",
    "        else:\n",
    "#             print(\"Cramming failed!\")\n",
    "            pass\n",
    "    \n",
    "    else:\n",
    "#         print(\"條件不合，不能Cramming\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizing(network):\n",
    "\n",
    "#     print(\"<<Regularizing module>>\")\n",
    "#     print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    ## Record the number of executions\n",
    "    times_enlarge = 0\n",
    "    times_shrink = 0\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "\n",
    "    ## Set epoch to 100\n",
    "    for i in range(100):\n",
    "\n",
    "        ## Store the parameter of the network\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "        loss_pre = loss\n",
    "        \n",
    "        ## Backward operation to obtain w'\n",
    "        network.backward_Adadelta(loss)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "\n",
    "        # Confirm whether the adjusted loss value is smaller than the current one\n",
    "        if loss <= loss_pre:\n",
    "            \n",
    "            ## Identify that all forecast value has met the error term\n",
    "            if torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "                \n",
    "                ## If true, multiply the learning rate by 1.2\n",
    "                network.learning_rate *= 1.2\n",
    "                times_enlarge += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                ## Else, restore w and end the process\n",
    "                network = copy.deepcopy(network_pre)\n",
    "#                 print(\"Regularizing結束-因為沒有顧好預測誤差\")\n",
    "#                 print(\"Number of enlarge:\",times_enlarge)\n",
    "#                 print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "        # If the adjusted loss value is not smaller than the current one\n",
    "        else:\n",
    "\n",
    "            ## If the learning rate is greater than the threshold for learning rate\n",
    "            if network.learning_rate > network.threshold_for_lr:\n",
    "                \n",
    "                ## Restore the w and multiply the learning rate by 0.7\n",
    "                network = copy.deepcopy(network_pre)\n",
    "                network.learning_rate *= 0.7\n",
    "                times_shrink += 1\n",
    "\n",
    "             ## If the learning rate is smaller than the threshold for learning rate\n",
    "            else:\n",
    "                \n",
    "                ## Restore the w\n",
    "                network = copy.deepcopy(network_pre)\n",
    "#                 print(\"Regularizing結束-Learning不能這麼小\")\n",
    "#                 print(\"Number of enlarge:\",times_enlarge)\n",
    "#                 print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "#     print(\"第\\\"%d\\\"回合Regularizing module完畢\"%(i+1))\n",
    "#     print(\"Number of enlarge:\",times_enlarge)\n",
    "#     print(\"Number of shrink:\",times_shrink)\n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganizing(network):\n",
    "#     print(\"<<Reorganizing module>>\")\n",
    "#     print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    limit = 4\n",
    "    if network.linear1.bias.shape[0] <= limit:\n",
    "        network = regularizing(network)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ## Set up the k = 1, and p = the number of hidden node\n",
    "        k = 1\n",
    "    #     p = network.W1.shape[1]\n",
    "        p = network.linear1.weight.data.shape[0]\n",
    "\n",
    "        while True:\n",
    "\n",
    "            ## If k > p, end of Process\n",
    "            if k > p or p<=limit:\n",
    "\n",
    "#                 print(\"Reorganizing result: The final number of neuro is \",p)\n",
    "                return(network)\n",
    "\n",
    "            ## Else, Process is ongoing\n",
    "            else:\n",
    "\n",
    "                ## Using the regularizing module to adjust the network\n",
    "                network = regularizing(network)\n",
    "\n",
    "                ## Store the network and w\n",
    "                network_pre = copy.deepcopy(network)\n",
    "\n",
    "                ## Set up the acceptable of the network as false\n",
    "                network.acceptable = False\n",
    "                \n",
    "            \n",
    "                ## Ignore the K hidden node\n",
    "                network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight[:k-1],network.linear1.weight[k:]],0))\n",
    "                network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias[:k-1],network.linear1.bias[k:]]))\n",
    "                network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight[:,:k-1],network.linear2.weight[:,k:]],1))\n",
    "\n",
    "                \n",
    "                ## Using the matching module to adjust the network\n",
    "                network = matching_for_reorganizing(network)\n",
    "\n",
    "#                 print(\"是不是可以不要這個hidden node:\",network.acceptable)\n",
    "\n",
    "                ## If the resulting network is acceptable, this means that the k hidden node can be removed\n",
    "                if network.acceptable:\n",
    "\n",
    "#                     print(\"Drop out the nero number: %d / %d\" %(k, p))\n",
    "                    network.nb_node_pruned += 1\n",
    "                    ## p--\n",
    "                    p-=1\n",
    "\n",
    "                ## Else, it means that the k hidden node cannot be removed\n",
    "                else:\n",
    "\n",
    "                    ## Restore the network and w\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "#                     print(\"Cannot drop out the nero number: %d / %d\" %(k, p))\n",
    "\n",
    "                    ## k++\n",
    "                    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <<1>> Window\n",
      "The training time(s): 95.75119376182556\n",
      "The forecast copper price for the fourth week: [[36090.01171875], [37372.3671875], [37444.59375], [37258.7265625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1725.4\n",
      "The standard deviation of absolute error: 1397.4\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 2218.3\n",
      "The accuracy(3000): 84.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<2>> Window\n",
      "The training time(s): 0.6220307350158691\n",
      "The forecast copper price for the fourth week: [[37877.33203125], [37846.53515625], [39673.078125], [39426.25390625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1623.6\n",
      "The standard deviation of absolute error: 1253.6\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 2049.5\n",
      "The accuracy(3000): 86.1%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<3>> Window\n",
      "The training time(s): 0.45551252365112305\n",
      "The forecast copper price for the fourth week: [[39926.890625], [41167.42578125], [42319.703125], [44560.734375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1670.7\n",
      "The standard deviation of absolute error: 1328.8\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 2132.8\n",
      "The accuracy(3000): 84.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<4>> Window\n",
      "The training time(s): 9.466110706329346\n",
      "The forecast copper price for the fourth week: [[40535.43359375], [40723.57421875], [41019.4921875], [41205.41015625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 2151.2\n",
      "The standard deviation of absolute error: 1387.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 2557.8\n",
      "The accuracy(3000): 74.5%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<5>> Window\n",
      "The training time(s): 36.21608066558838\n",
      "The forecast copper price for the fourth week: [[39513.828125], [39358.78515625], [39362.2890625], [39441.859375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1668.9\n",
      "The standard deviation of absolute error: 1276.5\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 2099.3\n",
      "The accuracy(3000): 85.6%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<6>> Window\n",
      "The training time(s): 37.65770864486694\n",
      "The forecast copper price for the fourth week: [[37436.375], [37717.24609375], [37701.859375], [37417.87109375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1306.9\n",
      "The standard deviation of absolute error: 1073.9\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1690.0\n",
      "The accuracy(3000): 92.6%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<7>> Window\n",
      "The training time(s): 42.97197437286377\n",
      "The forecast copper price for the fourth week: [[36670.6484375], [36473.7265625], [36286.64453125], [36291.0234375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1305.4\n",
      "The standard deviation of absolute error: 1067.8\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1684.9\n",
      "The accuracy(3000): 92.1%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<8>> Window\n",
      "The training time(s): 43.10417699813843\n",
      "The forecast copper price for the fourth week: [[37386.0703125], [37498.65234375], [37950.9296875], [38170.25]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1266.7\n",
      "The standard deviation of absolute error: 1058.6\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1649.3\n",
      "The accuracy(3000): 93.1%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<9>> Window\n",
      "The training time(s): 43.127591609954834\n",
      "The forecast copper price for the fourth week: [[38403.64453125], [38636.2109375], [38434.8125], [38062.00390625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1282.3\n",
      "The standard deviation of absolute error: 1049.4\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1655.4\n",
      "The accuracy(3000): 93.5%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<10>> Window\n",
      "The training time(s): 43.11757230758667\n",
      "The forecast copper price for the fourth week: [[37703.36328125], [37668.6953125], [37849.83984375], [37609.87109375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1267.4\n",
      "The standard deviation of absolute error: 1031.9\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1632.9\n",
      "The accuracy(3000): 93.1%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<11>> Window\n",
      "The training time(s): 42.8481330871582\n",
      "The forecast copper price for the fourth week: [[37793.7890625], [38531.99609375], [38408.8984375], [38847.9296875]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1252.6\n",
      "The standard deviation of absolute error: 1031.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1620.8\n",
      "The accuracy(3000): 93.5%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<12>> Window\n",
      "The training time(s): 43.010169982910156\n",
      "The forecast copper price for the fourth week: [[38997.14453125], [38872.09765625], [38780.94140625], [39216.46484375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1363.9\n",
      "The standard deviation of absolute error: 1316.6\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1893.6\n",
      "The accuracy(3000): 91.2%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<13>> Window\n",
      "The training time(s): 64.87540531158447\n",
      "The forecast copper price for the fourth week: [[40155.93359375], [41053.27734375], [41397.25390625], [40947.70703125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1279.8\n",
      "The standard deviation of absolute error: 1126.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1702.9\n",
      "The accuracy(3000): 91.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<14>> Window\n",
      "The training time(s): 86.24639105796814\n",
      "The forecast copper price for the fourth week: [[42146.0390625], [41958.2734375], [41962.9453125], [42070.85546875]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1316.2\n",
      "The standard deviation of absolute error: 1147.1\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1744.2\n",
      "The accuracy(3000): 89.8%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<15>> Window\n",
      "The training time(s): 86.21573901176453\n",
      "The forecast copper price for the fourth week: [[44047.890625], [44199.4296875], [44597.1640625], [44879.984375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1292.7\n",
      "The standard deviation of absolute error: 1061.5\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1671.1\n",
      "The accuracy(3000): 90.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<16>> Window\n",
      "The training time(s): 86.40681219100952\n",
      "The forecast copper price for the fourth week: [[46054.12890625], [46407.06640625], [46278.515625], [46106.328125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1213.6\n",
      "The standard deviation of absolute error: 996.8\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1569.0\n",
      "The accuracy(3000): 93.1%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<17>> Window\n",
      "The training time(s): 85.89972400665283\n",
      "The forecast copper price for the fourth week: [[46448.3828125], [46279.3125], [46041.68359375], [46038.56640625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1165.5\n",
      "The standard deviation of absolute error: 969.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1514.3\n",
      "The accuracy(3000): 94.0%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<18>> Window\n",
      "The training time(s): 86.05085229873657\n",
      "The forecast copper price for the fourth week: [[46744.49609375], [46714.11328125], [46599.58203125], [46414.15234375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1152.6\n",
      "The standard deviation of absolute error: 947.8\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1490.8\n",
      "The accuracy(3000): 94.4%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<19>> Window\n",
      "The training time(s): 85.83210778236389\n",
      "The forecast copper price for the fourth week: [[46252.90234375], [46000.859375], [45915.15625], [45829.84375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1115.6\n",
      "The standard deviation of absolute error: 946.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1461.3\n",
      "The accuracy(3000): 94.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<20>> Window\n",
      "The training time(s): 82.28228855133057\n",
      "The forecast copper price for the fourth week: [[45965.48046875], [46143.5078125], [46274.7890625], [45863.80859375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1092.3\n",
      "The standard deviation of absolute error: 942.1\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1441.0\n",
      "The accuracy(3000): 94.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<21>> Window\n",
      "The training time(s): 80.80136108398438\n",
      "The forecast copper price for the fourth week: [[45595.41015625], [45449.71484375], [45527.23828125], [45713.0546875]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1164.3\n",
      "The standard deviation of absolute error: 1029.5\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1552.6\n",
      "The accuracy(3000): 93.1%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<22>> Window\n",
      "The training time(s): 76.45094132423401\n",
      "The forecast copper price for the fourth week: [[46591.12109375], [46979.8984375], [47309.46484375], [47557.22265625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1206.5\n",
      "The standard deviation of absolute error: 1151.9\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1666.3\n",
      "The accuracy(3000): 90.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<23>> Window\n",
      "The training time(s): 76.83286905288696\n",
      "The forecast copper price for the fourth week: [[48469.61328125], [48826.4453125], [49087.44921875], [48867.73828125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1231.7\n",
      "The standard deviation of absolute error: 1137.7\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1674.9\n",
      "The accuracy(3000): 91.2%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<24>> Window\n",
      "The training time(s): 76.53065752983093\n",
      "The forecast copper price for the fourth week: [[49054.7734375], [48707.2890625], [49077.3671875], [49900.890625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1333.9\n",
      "The standard deviation of absolute error: 1239.1\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1818.7\n",
      "The accuracy(3000): 88.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<25>> Window\n",
      "The training time(s): 76.32692074775696\n",
      "The forecast copper price for the fourth week: [[50826.51953125], [51130.375], [50951.17578125], [50976.109375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1376.6\n",
      "The standard deviation of absolute error: 1184.3\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1814.1\n",
      "The accuracy(3000): 88.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<26>> Window\n",
      "The training time(s): 76.77872800827026\n",
      "The forecast copper price for the fourth week: [[51075.90234375], [50991.7578125], [50884.23828125], [50546.1015625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1417.6\n",
      "The standard deviation of absolute error: 1168.8\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1835.5\n",
      "The accuracy(3000): 88.0%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<27>> Window\n",
      "The training time(s): 76.7455780506134\n",
      "The forecast copper price for the fourth week: [[50860.94140625], [50761.21484375], [50758.1015625], [50948.203125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1432.1\n",
      "The standard deviation of absolute error: 1155.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1838.2\n",
      "The accuracy(3000): 89.4%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<28>> Window\n",
      "The training time(s): 77.73507237434387\n",
      "The forecast copper price for the fourth week: [[51919.0703125], [52135.6640625], [51873.8828125], [51651.0546875]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1403.7\n",
      "The standard deviation of absolute error: 1122.6\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1795.8\n",
      "The accuracy(3000): 90.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<29>> Window\n",
      "The training time(s): 77.40591645240784\n",
      "The forecast copper price for the fourth week: [[51242.83203125], [51476.56640625], [51926.89453125], [51909.75390625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1408.9\n",
      "The standard deviation of absolute error: 1103.6\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1788.1\n",
      "The accuracy(3000): 89.4%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<30>> Window\n",
      "The training time(s): 76.4130527973175\n",
      "The forecast copper price for the fourth week: [[52475.41015625], [52587.60546875], [51920.68359375], [50948.34765625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1412.0\n",
      "The standard deviation of absolute error: 1097.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1786.5\n",
      "The accuracy(3000): 89.8%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<31>> Window\n",
      "The training time(s): 77.89903950691223\n",
      "The forecast copper price for the fourth week: [[50879.8046875], [51020.046875], [51392.4609375], [51499.98046875]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1347.4\n",
      "The standard deviation of absolute error: 1068.5\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1718.1\n",
      "The accuracy(3000): 91.2%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<32>> Window\n",
      "The training time(s): 76.53320384025574\n",
      "The forecast copper price for the fourth week: [[51886.421875], [52411.546875], [51760.20703125], [51679.1796875]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1345.7\n",
      "The standard deviation of absolute error: 1068.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1716.5\n",
      "The accuracy(3000): 91.2%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<33>> Window\n",
      "The training time(s): 77.09227776527405\n",
      "The forecast copper price for the fourth week: [[51610.62109375], [51869.28515625], [52146.65234375], [52118.6015625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1366.0\n",
      "The standard deviation of absolute error: 1084.8\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1742.8\n",
      "The accuracy(3000): 90.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<34>> Window\n",
      "The training time(s): 75.86592602729797\n",
      "The forecast copper price for the fourth week: [[52137.3125], [51531.16015625], [50767.62890625], [50671.01953125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1381.6\n",
      "The standard deviation of absolute error: 1080.1\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1752.1\n",
      "The accuracy(3000): 89.8%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<35>> Window\n",
      "The training time(s): 76.09636688232422\n",
      "The forecast copper price for the fourth week: [[50435.76171875], [50310.32421875], [50325.90625], [50488.7421875]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1384.0\n",
      "The standard deviation of absolute error: 1071.3\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1748.6\n",
      "The accuracy(3000): 90.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<36>> Window\n",
      "The training time(s): 66.75917530059814\n",
      "The forecast copper price for the fourth week: [[50321.26171875], [50190.37109375], [49598.24609375], [49679.2734375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1381.3\n",
      "The standard deviation of absolute error: 1062.7\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1741.3\n",
      "The accuracy(3000): 89.8%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<37>> Window\n",
      "The training time(s): 66.66623616218567\n",
      "The forecast copper price for the fourth week: [[50152.9765625], [49777.44140625], [49732.25390625], [49971.44140625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1380.5\n",
      "The standard deviation of absolute error: 1062.7\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1740.7\n",
      "The accuracy(3000): 89.8%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<38>> Window\n",
      "The training time(s): 66.58221936225891\n",
      "The forecast copper price for the fourth week: [[50453.72265625], [50702.26171875], [50126.49609375], [49984.6953125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1383.3\n",
      "The standard deviation of absolute error: 1059.8\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1741.1\n",
      "The accuracy(3000): 90.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<39>> Window\n",
      "The training time(s): 66.62552833557129\n",
      "The forecast copper price for the fourth week: [[49606.8515625], [49183.79296875], [49063.80859375], [48990.5703125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1380.9\n",
      "The standard deviation of absolute error: 1053.1\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1735.2\n",
      "The accuracy(3000): 90.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<40>> Window\n",
      "The training time(s): 66.62226223945618\n",
      "The forecast copper price for the fourth week: [[48943.828125], [48527.00390625], [47940.328125], [47593.625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1380.8\n",
      "The standard deviation of absolute error: 1050.5\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1733.5\n",
      "The accuracy(3000): 90.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<41>> Window\n",
      "The training time(s): 66.72303533554077\n",
      "The forecast copper price for the fourth week: [[47279.65625], [46968.0078125], [46603.3828125], [46615.8515625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1409.9\n",
      "The standard deviation of absolute error: 1049.3\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1756.1\n",
      "The accuracy(3000): 90.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<42>> Window\n",
      "The training time(s): 66.52363443374634\n",
      "The forecast copper price for the fourth week: [[46714.8125], [47093.4609375], [47282.0078125], [47704.2890625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1388.8\n",
      "The standard deviation of absolute error: 1016.1\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1719.4\n",
      "The accuracy(3000): 91.2%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<43>> Window\n",
      "The training time(s): 66.66928434371948\n",
      "The forecast copper price for the fourth week: [[47841.49609375], [48040.16796875], [48305.84765625], [48422.71484375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1368.4\n",
      "The standard deviation of absolute error: 985.7\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1685.1\n",
      "The accuracy(3000): 91.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<44>> Window\n",
      "The training time(s): 66.88787412643433\n",
      "The forecast copper price for the fourth week: [[48568.41015625], [48920.56640625], [48849.66796875], [49077.94921875]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1366.4\n",
      "The standard deviation of absolute error: 989.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1685.4\n",
      "The accuracy(3000): 91.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<45>> Window\n",
      "The training time(s): 66.83494973182678\n",
      "The forecast copper price for the fourth week: [[49003.17578125], [48645.5625], [48340.15234375], [48280.9375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1369.1\n",
      "The standard deviation of absolute error: 988.8\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1687.5\n",
      "The accuracy(3000): 92.6%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<46>> Window\n",
      "The training time(s): 66.87627124786377\n",
      "The forecast copper price for the fourth week: [[47854.09375], [47792.54296875], [47487.91015625], [47385.84765625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1282.7\n",
      "The standard deviation of absolute error: 914.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1573.8\n",
      "The accuracy(3000): 94.4%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<47>> Window\n",
      "The training time(s): 66.91731548309326\n",
      "The forecast copper price for the fourth week: [[47500.4140625], [47603.2578125], [47503.53125], [47903.99609375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1269.8\n",
      "The standard deviation of absolute error: 912.4\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1562.4\n",
      "The accuracy(3000): 94.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<48>> Window\n",
      "The training time(s): 66.79387521743774\n",
      "The forecast copper price for the fourth week: [[48013.0703125], [48000.60546875], [47706.87890625], [47372.640625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1268.9\n",
      "The standard deviation of absolute error: 914.2\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1562.7\n",
      "The accuracy(3000): 94.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<49>> Window\n",
      "The training time(s): 66.6118061542511\n",
      "The forecast copper price for the fourth week: [[47147.4765625], [47026.71484375], [47050.08984375], [47496.51953125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1236.9\n",
      "The standard deviation of absolute error: 913.8\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1536.5\n",
      "The accuracy(3000): 94.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<50>> Window\n",
      "The training time(s): 66.5186996459961\n",
      "The forecast copper price for the fourth week: [[47735.70703125], [47551.05859375], [47546.3828125], [47429.515625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1219.8\n",
      "The standard deviation of absolute error: 910.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1520.6\n",
      "The accuracy(3000): 94.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<51>> Window\n",
      "The training time(s): 66.63504409790039\n",
      "The forecast copper price for the fourth week: [[47139.6875], [47548.71875], [47801.15234375], [48040.33984375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1207.2\n",
      "The standard deviation of absolute error: 913.7\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1512.7\n",
      "The accuracy(3000): 94.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<52>> Window\n",
      "The training time(s): 66.77689337730408\n",
      "The forecast copper price for the fourth week: [[47964.7734375], [47999.83203125], [48242.13671875], [47664.03515625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1212.8\n",
      "The standard deviation of absolute error: 910.2\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1515.1\n",
      "The accuracy(3000): 94.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<53>> Window\n",
      "The training time(s): 66.49001288414001\n",
      "The forecast copper price for the fourth week: [[47996.71875], [48059.046875], [48626.2421875], [48612.21484375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1188.6\n",
      "The standard deviation of absolute error: 897.1\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1487.9\n",
      "The accuracy(3000): 95.4%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<54>> Window\n",
      "The training time(s): 66.69287657737732\n",
      "The forecast copper price for the fourth week: [[48739.2109375], [48835.82421875], [48986.96875], [48440.03125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1226.6\n",
      "The standard deviation of absolute error: 932.2\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1539.3\n",
      "The accuracy(3000): 94.0%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<55>> Window\n",
      "The training time(s): 66.53161406517029\n",
      "The forecast copper price for the fourth week: [[47360.1796875], [47455.234375], [47041.5234375], [46672.22265625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1288.2\n",
      "The standard deviation of absolute error: 1147.5\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1723.4\n",
      "The accuracy(3000): 91.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<56>> Window\n",
      "The training time(s): 70.21611618995667\n",
      "The forecast copper price for the fourth week: [[40764.8046875], [45540.96484375], [44490.72265625], [42576.4375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1253.1\n",
      "The standard deviation of absolute error: 1047.9\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1632.0\n",
      "The accuracy(3000): 93.1%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<57>> Window\n",
      "The training time(s): 80.00786137580872\n",
      "The forecast copper price for the fourth week: [[41880.30859375], [41514.609375], [42321.87109375], [42011.78125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1240.5\n",
      "The standard deviation of absolute error: 1004.9\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1595.0\n",
      "The accuracy(3000): 93.5%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<58>> Window\n",
      "The training time(s): 80.22321486473083\n",
      "The forecast copper price for the fourth week: [[42211.0390625], [42590.859375], [42637.21484375], [42964.0546875]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1276.8\n",
      "The standard deviation of absolute error: 1045.5\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1648.7\n",
      "The accuracy(3000): 92.6%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<59>> Window\n",
      "The training time(s): 79.9954092502594\n",
      "The forecast copper price for the fourth week: [[43250.39453125], [43711.62890625], [44768.88671875], [45322.83984375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1365.2\n",
      "The standard deviation of absolute error: 1187.1\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1807.4\n",
      "The accuracy(3000): 93.1%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<60>> Window\n",
      "The training time(s): 80.30333137512207\n",
      "The forecast copper price for the fourth week: [[46582.671875], [46874.44921875], [47095.9140625], [47783.09375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1412.2\n",
      "The standard deviation of absolute error: 1237.1\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1875.5\n",
      "The accuracy(3000): 91.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<61>> Window\n",
      "The training time(s): 80.49616885185242\n",
      "The forecast copper price for the fourth week: [[49934.4375], [49955.47265625], [50025.984375], [49981.18359375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1277.1\n",
      "The standard deviation of absolute error: 1030.7\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1639.6\n",
      "The accuracy(3000): 93.5%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<62>> Window\n",
      "The training time(s): 80.38205170631409\n",
      "The forecast copper price for the fourth week: [[50854.578125], [50722.90625], [50469.6953125], [50652.0078125]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1268.6\n",
      "The standard deviation of absolute error: 999.4\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1613.6\n",
      "The accuracy(3000): 94.0%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<63>> Window\n",
      "The training time(s): 80.48890972137451\n",
      "The forecast copper price for the fourth week: [[50690.19140625], [50460.3515625], [49966.39453125], [49880.69140625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1240.7\n",
      "The standard deviation of absolute error: 970.2\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1573.6\n",
      "The accuracy(3000): 95.8%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<64>> Window\n",
      "The training time(s): 80.23046803474426\n",
      "The forecast copper price for the fourth week: [[49909.12890625], [50298.296875], [50154.16015625], [50175.9765625]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1300.6\n",
      "The standard deviation of absolute error: 1062.7\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1677.9\n",
      "The accuracy(3000): 95.4%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The <<65>> Window\n",
      "The training time(s): 80.81661224365234\n",
      "The forecast copper price for the fourth week: [[50621.6328125], [50800.05078125], [51124.9375], [52017.0234375]]\n",
      "<<Performance measurement>>\n",
      "The MAE: 1365.2\n",
      "The standard deviation of absolute error: 1201.0\n",
      "The MAPE: 0.0%\n",
      "The RMSE: 1816.4\n",
      "The accuracy(3000): 94.0%\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluation_table_train = pd.DataFrame(columns=[\"Window_index\",\"Stage\",\"MAE\",\"SD of absolute error\",\"MAPE\",\"RMSE\",\"Accuracy(3000)\",\"Step4\",\"Step6.1\",\"Step6.2\",\"Time\",\"Adopted_hidden_node\"])\n",
    "evaluation_table_test = pd.DataFrame(columns=[\"Window_index\",\"Stage\",\"MAE\",\"SD of absolute error\",\"MAPE\",\"RMSE\",\"Accuracy(3000)\",\"Step4\",\"Step6.1\",\"Step6.2\",\"Time\",\"Adopted_hidden_node\"])\n",
    "    \n",
    "x_data, y_data= get_data(4)\n",
    "x_data = sc.fit_transform(x_data)\n",
    "y_data = sc.fit_transform(y_data[:,3].reshape(-1,1))\n",
    "threshold_for_error = 7000/(sc.data_max_-sc.data_min_)\n",
    "\n",
    "data = range(x_data.shape[0])\n",
    "# window_size => the length of training block\n",
    "window_size = 212\n",
    "# step_window => step size of each window\n",
    "step_window = 4\n",
    "# the split data\n",
    "splits = []\n",
    "\n",
    "## Moving window mechnism\n",
    "for i in range(window_size, len(data), step_window):\n",
    "    train = np.array(data[i-window_size:i])\n",
    "    #test = np.array(data[i:i+step_window])\n",
    "    test = np.array(data[i-window_size:i+step_window])\n",
    "    splits.append(('TRAIN:', train, 'TEST:', test))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i_block in range(len(splits)):\n",
    "# for i_block in range(2):\n",
    "    block_start = time.time()\n",
    "    ## Record the number of each step\n",
    "    nb_step4 = 0\n",
    "    nb_step6_1 = 0\n",
    "    nb_step6_2 = 0\n",
    "    \n",
    "    print(\"The <<%d>> Window\" %(i_block+1))\n",
    "    \n",
    "    x_train = x_data[splits[i_block][1]]\n",
    "    x_test = x_data[splits[i_block][3]]\n",
    "    y_train = y_data[splits[i_block][1]]\n",
    "    y_test = y_data[splits[i_block][3]]\n",
    "    \n",
    "    x_train_scaled = torch.FloatTensor(x_train)\n",
    "    x_test_scaled = torch.FloatTensor(x_test)\n",
    "    y_train_scaled = torch.FloatTensor(y_train)\n",
    "    y_test = sc.inverse_transform(y_test)\n",
    "\n",
    "    if i_block == 0:\n",
    "        lower = torch.mean(y_train_scaled)-0.1*torch.std(y_train_scaled)\n",
    "        upper = torch.mean(y_train_scaled)+0.1*torch.std(y_train_scaled)\n",
    "        nonoutlier_index = torch.nonzero((y_train_scaled[:,0]>lower)&(y_train_scaled[:,0]<upper), as_tuple =False).reshape([-1])\n",
    "        \n",
    "        initial_x = x_train_scaled[nonoutlier_index[:19]]\n",
    "        initial_y = y_train_scaled[nonoutlier_index[:19]]\n",
    "        \n",
    "        x_train_scaled = np.delete(x_train_scaled, nonoutlier_index[:19], 0)\n",
    "        y_train_scaled = np.delete(y_train_scaled, nonoutlier_index[:19], 0)\n",
    "        \n",
    "        network = Network(1,initial_x,initial_y)\n",
    "        \n",
    "        network.nb_node_acceptable = torch.IntTensor([1 for _ in range(initial_x.shape[0])])\n",
    "        network.threshold_for_error = round(threshold_for_error[0],2)\n",
    "        \n",
    "        initializing(network, initial_x, initial_y)\n",
    "        \n",
    "#         print(\"<<Initializing後看一下差異>>\")\n",
    "#         yo,loss = network.forward()\n",
    "#         print(torch.abs(network.y-yo))\n",
    "#         print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "        nb_step4 += initial_x.shape[0]    \n",
    "        remainder = int(window_size) - initial_x.shape[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        sorted_index = selecting(network, x_train_scaled, y_train_scaled)\n",
    "        restart_index = int(x_train_scaled.shape[0])-step_window\n",
    "#         print(\"其他區塊剛開始選的資料索引：\",sorted_index[:restart_index])\n",
    "        init_x = x_train_scaled[sorted_index[:int(x_train_scaled.shape[0])-step_window]].reshape(-1,x_train_scaled.shape[1])\n",
    "        init_y = y_train_scaled[sorted_index[:int(x_train_scaled.shape[0])-step_window]].reshape(-1,1)\n",
    "        network.setData(init_x, init_y)\n",
    "        network.nb_node_acceptable = torch.IntTensor([network.linear1.bias.data.shape[0] for _ in range(init_x.shape[0])])\n",
    "        network.nb_node_pruned = 0\n",
    "        \n",
    "#         print(\"<<其他區塊剛開始時看一下差異>>\")\n",
    "#         yo,loss = network.forward()\n",
    "#         print(torch.abs(network.y-yo))\n",
    "        \n",
    "        nb_step4 += init_x.shape[0]    \n",
    "        remainder = int(window_size) - init_x.shape[0]\n",
    "        x_train_scaled = np.delete(x_train_scaled, sorted_index[:restart_index], 0)\n",
    "        y_train_scaled = np.delete(y_train_scaled, sorted_index[:restart_index], 0)\n",
    "        \n",
    "\n",
    "    for i in range(remainder):\n",
    "#    for i in range(3):\n",
    "        \n",
    "        sorted_index = selecting(network, x_train_scaled, y_train_scaled)\n",
    "        \n",
    "        ## Add new data for training     \n",
    "        network.addData(x_train_scaled[sorted_index[0]], y_train_scaled[sorted_index[0]])\n",
    "        x_train_scaled = np.delete(x_train_scaled, sorted_index[0], 0)\n",
    "        y_train_scaled = np.delete(y_train_scaled, sorted_index[0], 0)\n",
    "        \n",
    "        yo,loss = network.forward()\n",
    "        pre_network = copy.deepcopy(network)\n",
    "        \n",
    "        if not torch.all(torch.abs(network.y-yo)<=network.threshold_for_error):\n",
    "\n",
    "            network.acceptable = False\n",
    "            network = matching(network)\n",
    "            \n",
    "            \n",
    "            if network.acceptable == False:\n",
    "                \n",
    "                network = copy.deepcopy(pre_network)\n",
    "                cramming(network)\n",
    "\n",
    "                if network.acceptable == False:\n",
    "                    sys.exit(0)  \n",
    "                \n",
    "                nb_step6_2 += 1\n",
    "\n",
    "            else:\n",
    "                nb_step6_1 += 1\n",
    "\n",
    "        else:\n",
    "            nb_step4 += 1\n",
    "\n",
    "        network = reorganizing(network)\n",
    "        yo,loss = network.forward()\n",
    "        \n",
    "        network.nb_node_acceptable = torch.cat([network.nb_node_acceptable, torch.IntTensor([network.linear1.bias.data.shape[0]])],0)\n",
    "\n",
    "#     print(\"<<The performance of %d window>>\"%(i_block+1))\n",
    "    block_end = time.time()\n",
    "    evaluation_table_train, evaluation_table_test = validation(network, nb_step4, nb_step6_1, nb_step6_2, x_train_scaled, y_train_scaled,x_test_scaled, y_test, block_start, block_end,i_block+1,evaluation_table_train,evaluation_table_test)\n",
    "\n",
    "    evaluation_table_train.to_csv(\"evaluation_table_train.csv\",index=False)\n",
    "    evaluation_table_test.to_csv(\"evaluation_table_inferencing.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
